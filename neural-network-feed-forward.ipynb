{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://www.kaggle.com/oddrationale/mnist-in-csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...    28x19  28x20  \\\n",
       "0      5    0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "1      0    0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "2      4    0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "3      1    0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "4      9    0    0    0    0    0    0    0    0    0  ...        0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import dataset\n",
    "\n",
    "train_df =  pd.read_csv('data/mnist/train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.00392157, 0.00784314, 0.01176471, 0.03529412,\n",
       "       0.04313725, 0.05490196, 0.0627451 , 0.07058824, 0.09019608,\n",
       "       0.09411765, 0.09803922, 0.10196078, 0.10588235, 0.11764706,\n",
       "       0.1372549 , 0.14117647, 0.15294118, 0.16862745, 0.17647059,\n",
       "       0.18039216, 0.19215686, 0.21568627, 0.21960784, 0.25098039,\n",
       "       0.25882353, 0.2745098 , 0.30588235, 0.31372549, 0.31764706,\n",
       "       0.32156863, 0.35294118, 0.36470588, 0.36862745, 0.41960784,\n",
       "       0.42352941, 0.44705882, 0.46666667, 0.49411765, 0.49803922,\n",
       "       0.50980392, 0.51764706, 0.52156863, 0.52941176, 0.53333333,\n",
       "       0.54509804, 0.58039216, 0.58823529, 0.60392157, 0.61176471,\n",
       "       0.62745098, 0.65098039, 0.66666667, 0.67058824, 0.6745098 ,\n",
       "       0.68627451, 0.71372549, 0.71764706, 0.72941176, 0.73333333,\n",
       "       0.74509804, 0.76470588, 0.77647059, 0.78823529, 0.80392157,\n",
       "       0.81176471, 0.83137255, 0.83529412, 0.85882353, 0.86666667,\n",
       "       0.88235294, 0.88627451, 0.89803922, 0.93333333, 0.94117647,\n",
       "       0.94509804, 0.94901961, 0.95686275, 0.96862745, 0.97647059,\n",
       "       0.98039216, 0.98431373, 0.98823529, 0.99215686, 1.        ])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NORMALIZE = True\n",
    "\n",
    "if NORMALIZE:\n",
    "    train_df.loc[:, train_df.columns != 'label'] /= 255.0\n",
    "\n",
    "np.unique(train_df.loc[0, train_df.columns != 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Neural Network weights and structure\n",
    "\n",
    "The structure can be changed dynamically by changing the \"constants\" :\n",
    "- INPUT_SIZE\n",
    "- OUTPUT_SIZE\n",
    "- HIDDEN_LAYERS_NBR\n",
    "- HIDDEN_LAYERS_SIZE\n",
    "\n",
    "Important variables:\n",
    "- neurons\n",
    "- weights\n",
    "- biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup neural network constants\n",
    "\n",
    "INPUT_SIZE = 784\n",
    "OUTPUT_SIZE = 10\n",
    "\n",
    "HIDDEN_LAYERS_NBR = 1\n",
    "HIDDEN_LAYERS_SIZE = 32 # same size for all layers ( for simplification purposes )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neurons structure: ( there are 1 hidden layers)\n",
      "(784, 1)\n",
      "(32, 1)\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "# Construct Neurons\n",
    "def construct_neurons():\n",
    "    length = HIDDEN_LAYERS_NBR+2\n",
    "    ret = [None] * (length)\n",
    "    for i in range(length):\n",
    "        if i == 0: # First Layer is Input\n",
    "            ret[i] = np.zeros(shape=(INPUT_SIZE,1))\n",
    "        elif i == (length-1): # and Last layer is Output layer\n",
    "            ret[i] = np.zeros(shape=(OUTPUT_SIZE,1))\n",
    "        else: # Hidden Layers\n",
    "            ret[i] = np.zeros(shape=(HIDDEN_LAYERS_SIZE, 1))\n",
    "    return ret\n",
    "    \n",
    "\n",
    "neurons = construct_neurons()\n",
    "    \n",
    "\n",
    "print('Neurons structure: ( there are {} hidden layers)'.format(HIDDEN_LAYERS_NBR))\n",
    "for layer in range(len(neurons)):\n",
    "    print(neurons[layer].shape)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights structure: \n",
      "(None)\n",
      "(32, 784)\n",
      "(10, 32)\n",
      "[None, array([[0.01261317, 0.04044516, 0.03969592, ..., 0.02760895, 0.04212768,\n",
      "        0.04390514],\n",
      "       [0.01263063, 0.00874074, 0.02205129, ..., 0.0072362 , 0.00369698,\n",
      "        0.0060419 ],\n",
      "       [0.02190169, 0.01728697, 0.00762238, ..., 0.04109185, 0.02404104,\n",
      "        0.00403787],\n",
      "       ...,\n",
      "       [0.0275615 , 0.01595086, 0.00689466, ..., 0.00214547, 0.04591417,\n",
      "        0.00651962],\n",
      "       [0.02317522, 0.00660563, 0.02174807, ..., 0.04529433, 0.00205456,\n",
      "        0.0347554 ],\n",
      "       [0.0234824 , 0.01672635, 0.04095114, ..., 0.0392324 , 0.01671102,\n",
      "        0.01755148]]), array([[0.02288898, 0.16030559, 0.19943867, 0.06293118, 0.06062961,\n",
      "        0.23196431, 0.02719215, 0.14442328, 0.20437581, 0.05249125,\n",
      "        0.23209503, 0.05396892, 0.18167084, 0.01240049, 0.21351511,\n",
      "        0.07262554, 0.23859753, 0.23743804, 0.09193241, 0.14953049,\n",
      "        0.10098255, 0.04914516, 0.16935215, 0.22253735, 0.23181848,\n",
      "        0.15965574, 0.24479028, 0.18412704, 0.08110401, 0.1402845 ,\n",
      "        0.19435048, 0.22455033],\n",
      "       [0.18105047, 0.12371642, 0.07205365, 0.09599038, 0.23590109,\n",
      "        0.07754681, 0.11362026, 0.10214444, 0.19566413, 0.01115284,\n",
      "        0.10026589, 0.03710847, 0.11880797, 0.00638109, 0.0938451 ,\n",
      "        0.14336435, 0.01537954, 0.08458898, 0.21393314, 0.06441813,\n",
      "        0.2171736 , 0.03170547, 0.10694296, 0.19219982, 0.18784807,\n",
      "        0.2450917 , 0.24865231, 0.13667756, 0.11698738, 0.14349926,\n",
      "        0.09683087, 0.21159801],\n",
      "       [0.13690916, 0.13927576, 0.17846249, 0.11495323, 0.10941353,\n",
      "        0.1832661 , 0.02089171, 0.12180234, 0.19468331, 0.09707709,\n",
      "        0.24176661, 0.11222834, 0.20392592, 0.04561012, 0.07477243,\n",
      "        0.1476698 , 0.09346488, 0.1715663 , 0.17342953, 0.0329169 ,\n",
      "        0.21557155, 0.22309202, 0.07093093, 0.0302756 , 0.0451534 ,\n",
      "        0.21163584, 0.21289733, 0.15267849, 0.11354465, 0.19464631,\n",
      "        0.12728412, 0.15434515],\n",
      "       [0.20442034, 0.14190029, 0.16078049, 0.16414055, 0.16023342,\n",
      "        0.03933111, 0.13850818, 0.19339878, 0.21749371, 0.04228818,\n",
      "        0.23799123, 0.24448394, 0.12484251, 0.09395961, 0.1067959 ,\n",
      "        0.20286294, 0.16253487, 0.05168758, 0.05345059, 0.24358105,\n",
      "        0.12962029, 0.1495275 , 0.07048452, 0.1287583 , 0.04224891,\n",
      "        0.09695839, 0.18908582, 0.12295242, 0.2143538 , 0.13647899,\n",
      "        0.18360837, 0.12351672],\n",
      "       [0.24670877, 0.21245966, 0.17521996, 0.03916691, 0.15912806,\n",
      "        0.24422955, 0.1732893 , 0.20261722, 0.12110436, 0.09357706,\n",
      "        0.05187935, 0.211046  , 0.1760101 , 0.00741184, 0.07198867,\n",
      "        0.08855493, 0.19026126, 0.0980885 , 0.04416927, 0.16620489,\n",
      "        0.22634789, 0.15326107, 0.22197541, 0.13380589, 0.1104085 ,\n",
      "        0.2305512 , 0.17431326, 0.23824268, 0.03524044, 0.11535305,\n",
      "        0.09805194, 0.03026768],\n",
      "       [0.07816284, 0.09546758, 0.16747272, 0.1986165 , 0.23543737,\n",
      "        0.19797356, 0.07833241, 0.24678754, 0.2125091 , 0.03088359,\n",
      "        0.11325072, 0.18374929, 0.05709336, 0.20638975, 0.20377146,\n",
      "        0.05736882, 0.16539535, 0.24648243, 0.13245466, 0.13305659,\n",
      "        0.01086029, 0.22622029, 0.12351061, 0.066072  , 0.13484025,\n",
      "        0.15935944, 0.04169813, 0.09902529, 0.19883039, 0.00504135,\n",
      "        0.12999965, 0.10677355],\n",
      "       [0.13510064, 0.18272366, 0.14065367, 0.04654475, 0.19950017,\n",
      "        0.17712377, 0.11574234, 0.24562965, 0.20766847, 0.11773838,\n",
      "        0.05525331, 0.14960195, 0.20823813, 0.02579159, 0.18441863,\n",
      "        0.23813925, 0.11852694, 0.06625359, 0.04485462, 0.21144097,\n",
      "        0.20227281, 0.03615319, 0.02275305, 0.18387121, 0.07874251,\n",
      "        0.00645349, 0.04552723, 0.05192116, 0.20480495, 0.24745574,\n",
      "        0.18521552, 0.21689556],\n",
      "       [0.15517429, 0.21599296, 0.16875493, 0.16710093, 0.15392637,\n",
      "        0.08727177, 0.08738413, 0.20482571, 0.08868792, 0.13434101,\n",
      "        0.15511796, 0.15775209, 0.00454353, 0.06715359, 0.10805571,\n",
      "        0.03708961, 0.09732121, 0.18491186, 0.18806657, 0.21033806,\n",
      "        0.21703852, 0.13369849, 0.20365687, 0.22666699, 0.12719636,\n",
      "        0.00880673, 0.01145602, 0.17427337, 0.18970018, 0.20150972,\n",
      "        0.05768085, 0.02561272],\n",
      "       [0.07913603, 0.18024279, 0.24960451, 0.04756894, 0.07124323,\n",
      "        0.19885448, 0.2100757 , 0.18570372, 0.10736283, 0.14321746,\n",
      "        0.10645323, 0.01933487, 0.10593115, 0.16344876, 0.03993671,\n",
      "        0.00390781, 0.17609162, 0.1288642 , 0.22047349, 0.1852591 ,\n",
      "        0.0928968 , 0.02380809, 0.11168172, 0.06455828, 0.12608508,\n",
      "        0.23401187, 0.24317577, 0.13331899, 0.04478495, 0.20266449,\n",
      "        0.22501365, 0.20066147],\n",
      "       [0.00329595, 0.22286648, 0.22569658, 0.09167424, 0.01665712,\n",
      "        0.15817835, 0.04662649, 0.15146568, 0.20058012, 0.16164861,\n",
      "        0.08300451, 0.22236171, 0.21958053, 0.14369416, 0.19049841,\n",
      "        0.04452106, 0.18359027, 0.11512085, 0.21267721, 0.11862723,\n",
      "        0.13954313, 0.13045981, 0.15058525, 0.22055804, 0.17096489,\n",
      "        0.24999172, 0.07559569, 0.10805211, 0.03133553, 0.19906566,\n",
      "        0.12227848, 0.18836635]])]\n"
     ]
    }
   ],
   "source": [
    "# Init Weights ( Xavier Initialization )\n",
    "\n",
    "def construct_weights(xavier_constant=2):  # When using RELU initialize weights with (rand) * sqrt(2/N)\n",
    "    length = HIDDEN_LAYERS_NBR + 2\n",
    "    ret = [None] * (length)\n",
    "    for i in range(1, length):\n",
    "        if i == 1: # the first weights between input and first hidden layer have structure : (HIDDEN_LAYERS_SIZE x INPUT_SIZE )\n",
    "            ret[i] = np.random.rand(HIDDEN_LAYERS_SIZE, INPUT_SIZE)*(np.sqrt(xavier_constant/INPUT_SIZE))\n",
    "        elif i == (length - 1): # last weights between last hidden layer and output have structure : (OUTPUT_SIZE x HIDDEN_LAYERS_SIZE)\n",
    "            ret[i] = np.random.rand(OUTPUT_SIZE, HIDDEN_LAYERS_SIZE)*(np.sqrt(xavier_constant/HIDDEN_LAYERS_SIZE))\n",
    "        else: # weights between hidden layers have structure (HIDDEN_LAYERS_SIZE x HIDDEN_LAYERS_SIZE)\n",
    "            ret[i] = np.random.rand(HIDDEN_LAYERS_SIZE, HIDDEN_LAYERS_SIZE)*(np.sqrt(xavier_constant/HIDDEN_LAYERS_SIZE))\n",
    "    return ret\n",
    "\n",
    "\n",
    "weights = construct_weights()\n",
    "\n",
    "print('Weights structure: ')\n",
    "for w in weights:\n",
    "    print(w.shape if w is not None else '(None)' )\n",
    "\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biases structure: \n",
      "(None)\n",
      "(32, 1)\n",
      "(10, 1)\n",
      "[None, array([[0.38227819],\n",
      "       [0.60495759],\n",
      "       [0.21125631],\n",
      "       [0.36084706],\n",
      "       [0.97133935],\n",
      "       [0.36777016],\n",
      "       [0.57179479],\n",
      "       [0.58634425],\n",
      "       [0.61708984],\n",
      "       [0.08667462],\n",
      "       [0.52182288],\n",
      "       [0.30551015],\n",
      "       [0.22032756],\n",
      "       [0.46081386],\n",
      "       [0.7446659 ],\n",
      "       [0.26889522],\n",
      "       [0.92042552],\n",
      "       [0.98697223],\n",
      "       [0.15202051],\n",
      "       [0.11796128],\n",
      "       [0.75667718],\n",
      "       [0.64591168],\n",
      "       [0.17093621],\n",
      "       [0.34886888],\n",
      "       [0.834263  ],\n",
      "       [0.5717872 ],\n",
      "       [0.5140515 ],\n",
      "       [0.14326781],\n",
      "       [0.52028372],\n",
      "       [0.63223854],\n",
      "       [0.70620955],\n",
      "       [0.17232377]]), array([[0.2461453 ],\n",
      "       [0.47167015],\n",
      "       [0.54173028],\n",
      "       [0.93267508],\n",
      "       [0.17721781],\n",
      "       [0.00603961],\n",
      "       [0.07413794],\n",
      "       [0.16436659],\n",
      "       [0.0564843 ],\n",
      "       [0.7972318 ]])]\n"
     ]
    }
   ],
   "source": [
    "def construct_biases():\n",
    "    length = len(neurons)\n",
    "    ret = [None] * (length)\n",
    "    for i in range(1, length):\n",
    "        ret[i] = np.random.rand(neurons[i].shape[0], 1)\n",
    "    return ret\n",
    "\n",
    "biases = construct_biases()\n",
    "\n",
    "print('Biases structure: ')\n",
    "for b in biases:\n",
    "    print(b.shape if b is not None else '(None)')\n",
    "\n",
    "print(biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Functions\n",
    "\n",
    "def sigmoid(x, prime=False):\n",
    "    if not prime:\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    else:\n",
    "        return sigmoid(X)*(1-sigmoid(X))\n",
    "\n",
    "def relu(x, prime=False):\n",
    "    if not prime:\n",
    "        return max(0,x)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "def activation(x, func='relu', prime=False, vectorized=False):\n",
    "    if func == 'relu':\n",
    "        return relu(x, prime) if not vectorized else np.array([relu(xi, prime) for xi in x]).reshape(-1,1)\n",
    "    elif func == 'sigmoid':\n",
    "        return sigmoid(x, prime) if not vectorized else np.array([sigmoid(xi, prime) for xi in x]).reshape(-1,1)\n",
    "    elif func == 'softmax' and vectorized:\n",
    "        exps = [np.exp(xi) for xi in x]\n",
    "        s = sum(exps)\n",
    "        ret = [e/s for e in exps]\n",
    "        return np.array(ret).reshape(-1,1)\n",
    "    else: \n",
    "        raise NameError('Unknown Activation Function Name')\n",
    "\n",
    "#v = np.array([4,100,4,12]).reshape(-1,1)\n",
    "#print(v)\n",
    "\n",
    "#res = activation(t, func='softmax', vectorized=True)\n",
    "#assert sum(res) == 1\n",
    "#res\n",
    "#print(np.random.rand(4,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.173965  ]\n",
      " [0.02519079]\n",
      " [0.08182588]\n",
      " [0.22669991]\n",
      " [0.10090713]\n",
      " [0.05581168]\n",
      " [0.04683737]\n",
      " [0.03624999]\n",
      " [0.03966829]\n",
      " [0.21284395]]\n"
     ]
    }
   ],
   "source": [
    "def feed_forward(input_vec):\n",
    "    global neurons\n",
    "    global t\n",
    "\n",
    "    neurons[0] = input_vec\n",
    "    for i in range(1,len(neurons)):\n",
    "        last_layer = (i == len(neurons)-1)\n",
    "        z = np.dot(weights[i], neurons[i-1]).reshape(-1,1) + biases[i]\n",
    "        neurons[i] = activation(z, func='relu' if not last_layer else 'softmax', vectorized=True)\n",
    "\n",
    "    return neurons[-1] # Return last layer (output)\n",
    "\n",
    "# Test\n",
    "\n",
    "test_vec = train_df.loc[5, train_df.columns != 'label']\n",
    "test_res = feed_forward(test_vec)\n",
    "print(test_res)\n",
    "assert sum(test_res) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back-Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Train Network (Online)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score the Network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
